{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_size: 4\n",
      "action_size: 2\n",
      "batch_size: 64\n",
      "learning_rate: 0.001\n",
      "gamma: 0.95\n",
      "memory_size: 100000\n",
      "epsilon: 1.0\n",
      "epsilon_decay: 0.995\n",
      "epsilon_min: 0.01\n",
      "num_episodes: 1000\n",
      "update_target_network_freq: 10\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "gamma = 0.95  # Discount rate\n",
    "memory_size = 100000  # Size of the replay buffer\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "num_episodes = 1000\n",
    "update_target_network_freq = 10  # Frequency to update target network\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Print all hyperparameters\n",
    "print('state_size:', state_size)\n",
    "print('action_size:', action_size)\n",
    "print('batch_size:', batch_size)\n",
    "print('learning_rate:', learning_rate)\n",
    "print('gamma:', gamma)\n",
    "print('memory_size:', memory_size)\n",
    "print('epsilon:', epsilon)\n",
    "print('epsilon_decay:', epsilon_decay)\n",
    "print('epsilon_min:', epsilon_min)\n",
    "print('num_episodes:', num_episodes)\n",
    "print('update_target_network_freq:', update_target_network_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "def build_model(hidden_layers,num_layers):\n",
    "    # Neural network for Deep Q Learning\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(hidden_layers, input_dim=state_size, activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(hidden_layers, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Create main model and target model\n",
    "main_model = build_model(32,3)\n",
    "target_model = build_model(32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the target network\n",
    "def update_target_model():\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "\n",
    "# Function to choose an action using epsilon-greedy policy\n",
    "def choose_action(state, epsilon):\n",
    "    return random.randrange(action_size) if np.random.rand() <= epsilon else np.argmax(main_model.predict(state)[0])\n",
    "\n",
    "# Function to store experience in replay memory\n",
    "def store_transition(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = 1000\n",
    "\n",
    "# Function to train the agent with a sampled batch from the replay buffer\n",
    "def train_replay():\n",
    "    if len(memory) < train_start:\n",
    "        return\n",
    "    # Sample a mini-batch from the memory\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    # Variables to store mini-batch data\n",
    "    update_input = np.zeros((batch_size, state_size))\n",
    "    update_target = np.zeros((batch_size, state_size))\n",
    "    action, reward, done = [], [], []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        update_input[i] = minibatch[i][0]\n",
    "        action.append(minibatch[i][1])\n",
    "        reward.append(minibatch[i][2])\n",
    "        update_target[i] = minibatch[i][3]\n",
    "        done.append(minibatch[i][4])\n",
    "    \n",
    "    # Set the target Q-value\n",
    "    target = main_model.predict(update_input)\n",
    "    target_next = main_model.predict(update_target)\n",
    "    target_val = target_model.predict(update_target)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # If the episode is done, just use the observed reward\n",
    "        if done[i]:\n",
    "            target[i][action[i]] = reward[i]\n",
    "        else:\n",
    "            # The key DDQN update step: use the main network to select the action,\n",
    "            # and the target network to compute the Q-value for that action\n",
    "            a = np.argmax(target_next[i])\n",
    "            target[i][action[i]] = reward[i] + gamma * target_val[i][a]\n",
    "\n",
    "    # Train the main model with the target Q-value\n",
    "    main_model.fit(update_input, target, batch_size=batch_size, epochs=1, verbose=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRLVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
